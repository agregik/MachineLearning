{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глава 6: Работа с текстом\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неструктурированные текстовые данные, такие как содержимое книги или твита являются одним из самых интересных источников признаков и одним из самых сложных для обработки. \n",
    "\n",
    "В этой главе я рассмотрю стратегии преобразования текста в информационно богатые признаки. \n",
    "\n",
    "Это не означает, что все эти примеры являются всеобъемлющими. Существуют целые академические дисциплины, ориентированные на обработку этого и подобных типов данных и содержимое всех их методов заполнит небольшую библиотеку. \n",
    "\n",
    "Несмотря на это есть часто используемые методы, и овладение ими добавит ценные инструменты. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1 Очистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**: Даны некоторые неструктурированные текстовые данные и требуется выполнить их элементарную очистку. \n",
    "\n",
    "**Решение**: Большинство элементарных операций очистки текста соответствуют элементарным операциям языка Python над строковыми значениями, в частности `strip`, `replace` и `split`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет, меня зовут Никита',\n",
       " 'Сегодня я продуктивный',\n",
       " 'Завтра надеюсь быть таким же!']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создам какой-нибудь текст \n",
    "text = ['Привет, меня зовут Никита.     ',\n",
    "        'Сегодня я продуктивный.',\n",
    "        '    Завтра надеюсь быть таким же!']\n",
    "\n",
    "# Уберу пробелы\n",
    "strip_whitspace = [string.strip() for string in text]\n",
    "\n",
    "strip_whitspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Привет, меня зовут Никита',\n",
       " 'Сегодня я продуктивный',\n",
       " 'Завтра надеюсь быть таким же!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Теперь удалю точки\n",
    "without_dots = [string.replace('.', '') for string in strip_whitspace]\n",
    "without_dots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также мы обычно применяем свои функции для манипулции с текстом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция которая переводит слова в верхний регистр\n",
    "def capitalizer(string:str) -> str:\n",
    "    return string.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ПРИВЕТ, МЕНЯ ЗОВУТ НИКИТА',\n",
       " 'СЕГОДНЯ Я ПРОДУКТИВНЫЙ',\n",
       " 'ЗАВТРА НАДЕЮСЬ БЫТЬ ТАКИМ ЖЕ!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[capitalizer(string) for string in without_dots]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для более мощных операций можно использовать регулярки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Понятно по названию что буду менять все буквы на ДАБЛЙУ\n",
    "def replace_letters_with_W(string:str) -> str:\n",
    "    return re.sub(r'[а-яА-Я]', 'W', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WWWWWW, WWWW WWWWW WWWWWW',\n",
       " 'WWWWWWW W WWWWWWWWWWWW',\n",
       " 'WWWWWW WWWWWWW WWWW WWWWW WW!']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[replace_letters_with_W(string) for string in without_dots]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обсуждение**:\n",
    "\n",
    "Большинство текстовых данных следует очистить перед тем как их применять для построения признаков. \n",
    "\n",
    "Подавляющую часть элементарной очистки текста можно выполнять с помощью стандартных функций Python.\n",
    "\n",
    "В реальном мире мы скорее всего сами определим функцию очистки (например `capitalizer`), объединяющую несколько задач очистки и применим её к текстовым данным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дополнительные материалы для чтения**:\n",
    "- \"Практическое руководство по регулярным выражениям в Python для начинаю­щих\", ресурс для аналитиков данных Analytics Vidhya (http://bit.ly/2HTGZuu). Если не пускает - VPN или другой ресурс =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Разбор и очистка разметки HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**: Даны текстовые данные с элементами HTML и требуется извлечь только текст. \n",
    "\n",
    "**Решение**: Использовать обширный функционал библиотеки BeautifulSoup для анализа и извлечение текста из HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n       Nikita Anka '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загружу библиотеку\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Сделаю немного кода на HTML\n",
    "html = ''' \n",
    "       <div class = 'full_name'><span style = 'font-weight:bold'>\n",
    "       Nikita</span> Anka </div>\"\n",
    "'''\n",
    "\n",
    "# Выполню разбор HTML\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Найду элемент div с классом 'full_name' и покажу текст\n",
    "soup.find('div', {'class': 'full_name'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обсуждение**:\n",
    "\n",
    "Несмотря на странное название Beautiful Soup - мощная библиотека в Python, предназначенная для 'выскабливания' HTML.\n",
    "\n",
    "Как правило эта библиотека используется для вычищения живых веб-сайтов, но её можно также легко применять для извлечения текстовых данных, встроенных в HTML. \n",
    "\n",
    "Полный спектр операций биб­лиотеки Beautiful Soup выходит за рамки этой книги (проходил её в вузе), но даже те несколько методов, примененных в нашем решении, показывают, как легко можно проанализировать разметку HTML для извлечения данных, которые нам необходимы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Дополнительные материалы для чтения**:\n",
    "- Веб-сайт библиотеки Beautiful Soup (http://bit.ly/2pwZcYs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 Удаление знаков препинания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**: Дан признак в виде текстовых данных, и требуется удалить знаки препинания.\n",
    "\n",
    "**Решение**: Определить функцию, которая использует метод `translate` со словарем знаков пре­пинания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "# Сделал текст\n",
    "text_data = ['Hi!!!! I. Love. This. Song....', \n",
    "             '10000% Agree! !! !, #LoveIT', \n",
    "             'Right?!?!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаю словарь знаков препинания \n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                            if unicodedata.category(chr(i)).startswith('P'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree   LoveIT', 'Right']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удаляю любые знаки препинания во всех строковых значениях \n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обсуждение**:\n",
    "\n",
    "Метод `translate` языка Python популярен благодаря своей невероятной скорости. В моем решении сначала мы создали словарь `punctuation`, в котором в качестве ключей размещены все знаки препинания, присутствующие в Юникоде, и в качест­ве значений — None. \n",
    "\n",
    "Затем я преобразовал все символы строкового значения, которые находятся среди знаков препинания, в None, фактически удалив их. Существуют более читаемые способы удаления знаков препинания, но это несколько хакерское решение имеет преимущество в том, что оно гораздо быстрее, чем другие альтернативы.\n",
    "\n",
    "Важно осознавать тот факт, что знаки препинания содержат информацию (напри­мер, сравните \"правильно?\" и \"правильно!\"). Удаление знаков препинания часто является необходимым злом для создания признаков; однако, если знаки препина­ния важны, мы должны обязательно принимать их во внимание."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Лексемизация текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**: Дан текст и требуется разбить его на отдельные слова\n",
    "\n",
    "**Решение**: Комплект естественно-языковых инструментов NLTK (Natural Language Toolkit for Python) имеет мощный набор операций над текстом, включая лексемизацию на слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 22:05:20.417 Python[32217:5292872] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузить библиотеку\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Создать текст\n",
    "string = \"Сегодняшняя наука — это технология завтрашнего дня\"\n",
    "# Лексемизировать на слова \n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
